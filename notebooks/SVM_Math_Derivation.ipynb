{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "876c48ec",
   "metadata": {},
   "source": [
    "# Метод опорных векторов (SVM): математика и реализация с нуля\n",
    "\n",
    "В этом ноутбуке мы разберём **метод опорных векторов (Support Vector Machine, SVM)** — один из самых фундаментальных алгоритмов машинного обучения.\n",
    "\n",
    "- **Цель**: найти **оптимальную разделяющую гиперплоскость**.\n",
    "- **Ключевая идея**: **максимизировать зазор (margin)** между классами.\n",
    "- **Опорные вектора**: точки, ближайшие к границе — они **определяют положение гиперплоскости**.\n",
    "\n",
    "Реализуем **soft-margin SVM** с градиентным спуском на чистом NumPy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0c924",
   "metadata": {},
   "source": [
    "## 1. Формулировка задачи\n",
    "\n",
    "Рассматриваем **бинарную классификацию**:\n",
    "- Объект: $\\mathbf{x} \\in \\mathbb{R}^d$\n",
    "- Метка: $y \\in \\{-1, +1\\}$\n",
    "\n",
    "Модель предсказывает:\n",
    "$$\n",
    "F(\\mathbf{x}) = \\text{sign}(\\mathbf{w}^\\top \\mathbf{x} - b)\n",
    "$$\n",
    "\n",
    "Наша задача — найти такие $\\mathbf{w}$ и $b$, чтобы:\n",
    "- Все объекты были **правильно классифицированы**,\n",
    "- **Зазор между классами был максимальным**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d33aea",
   "metadata": {},
   "source": [
    "## 2. Зазор (Margin) и опорные вектора\n",
    "\n",
    "Для объекта $(\\mathbf{x}_i, y_i)$ **отступ (margin)** определяется как:\n",
    "$$\n",
    "M_i = y_i (\\mathbf{w}^\\top \\mathbf{x}_i - b)\n",
    "$$\n",
    "\n",
    "Интерпретация:\n",
    "- $M_i > 1$: объект **правильно классифицирован** и **вне разделяющей полосы**,\n",
    "- $0 < M_i < 1$: объект **внутри полосы**, но с правильным знаком,\n",
    "- $M_i \\leq 0$: объект **ошибочно классифицирован**.\n",
    "\n",
    "**Опорные вектора** — это объекты, для которых $M_i < 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007ff8c6",
   "metadata": {},
   "source": [
    "## 3. Жёсткий зазор (Hard-Margin SVM)\n",
    "\n",
    "Если данные **линейно разделимы**, решается задача:\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dfrac{1}{2} \\|\\mathbf{w}\\|^2 \\to \\min \\\\\n",
    "y_i (\\mathbf{w}^\\top \\mathbf{x}_i - b) \\geq 1, \\quad \\forall i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Это задача **квадратичного программирования (QP)**.  \n",
    "Решается через **двойственную функцию Лагранжа** → **метод SMO** (используется в `sklearn`).\n",
    "\n",
    "**Недостаток**: не работает при **шуме** или **перекрытии классов**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42510a56",
   "metadata": {},
   "source": [
    "## 4. Мягкий зазор (Soft-Margin SVM)\n",
    "\n",
    "Для **реальных данных** вводим **слабые ограничения** через **переменные зазора** $\\xi_i \\geq 0$:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\dfrac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^m \\xi_i \\to \\min \\\\\n",
    "y_i (\\mathbf{w}^\\top \\mathbf{x}_i - b) \\geq 1 - \\xi_i, \\quad \\forall i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- $C > 0$ — гиперпараметр: **баланс между максимальным зазором и допустимыми ошибками**.\n",
    "- Чем больше $C$, тем **меньше ошибок допускается** (ближе к hard-margin).\n",
    "\n",
    "Эту задачу можно **аппроксимировать** через **функцию потерь**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0c0412",
   "metadata": {},
   "source": [
    "## 5. Функция потерь: Hinge Loss\n",
    "\n",
    "Мягкий зазор эквивалентен минимизации **безусловной функции**:\n",
    "$$\n",
    "Q(\\mathbf{w}) = \\frac{1}{2} \\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^m \\max(0, 1 - y_i (\\mathbf{w}^\\top \\mathbf{x}_i - b))\n",
    "$$\n",
    "\n",
    "- $\\max(0, 1 - M_i)$ — **Hinge Loss**.\n",
    "- Если $M_i \\geq 1$ → $H = 0$ → **градиенты не текут**.\n",
    "- Если $M_i < 1$ → $H > 0$ → **веса обновляются**.\n",
    "\n",
    "Это позволяет использовать **градиентный спуск**!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa74588",
   "metadata": {},
   "source": [
    "## 6. Градиенты для обновления весов\n",
    "\n",
    "Градиент функции потерь по $\\mathbf{w}$:\n",
    "$$\n",
    "\\nabla Q =\n",
    "\\begin{cases}\n",
    "\\mathbf{w} - C \\cdot y_i \\mathbf{x}_i, & \\text{если } y_i (\\mathbf{w}^\\top \\mathbf{x}_i - b) < 1 \\\\\n",
    "\\mathbf{w}, & \\text{иначе}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "В нашей реализации:\n",
    "- $\\alpha = 1/C$ (коэффициент регуляризации),\n",
    "- Обновление: $\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\cdot \\nabla Q$,\n",
    "- где $\\eta$ — **скорость обучения (learning rate)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ed474",
   "metadata": {},
   "source": [
    "## 7. Почему Hinge Loss максимизирует зазор?\n",
    "\n",
    "- Градиенты текут **только для объектов с $M_i < 1$** — то есть для **опорных векторов**.\n",
    "- Объекты с $M_i \\geq 1$ **не влияют на обучение** → веса не меняются.\n",
    "- Таким образом, алгоритм «заботится» **только о ближайших к границе точках** — это и есть суть SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7c87dc",
   "metadata": {},
   "source": [
    "## 8. Отличия от логистической регрессии\n",
    "\n",
    "| Характеристика        | SVM                          | Логистическая регрессия      |\n",
    "|----------------------|------------------------------|-------------------------------|\n",
    "| Функция потерь       | Hinge Loss: $\\max(0, 1 - M)$ | Log Loss: $\\log(1 + e^{-M})$ |\n",
    "| Выход                | Метка класса ($\\pm1$)        | Вероятность                   |\n",
    "| Интерпретация        | Максимизация зазора          | Максимизация правдоподобия   |\n",
    "| Опорные объекты      | Только опорные вектора       | Все объекты влияют            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe03b85",
   "metadata": {},
   "source": [
    "## 9. Заключение\n",
    "\n",
    "- Мы реализовали **soft-margin SVM** через **градиентный спуск**.\n",
    "- Код не решает QP-задачу, но **аппроксимирует её** через Hinge Loss.\n",
    "- Это **практичный и обучающий подход**, позволяющий понять **суть SVM \"под капотом\"**.\n",
    "\n",
    "Полный код реализации — в `src/manually_svm.py`.  \n",
    "Демонстрация на данных Ирисов — в `examples/demo_svm.py`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
